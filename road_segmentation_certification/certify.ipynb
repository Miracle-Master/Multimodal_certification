{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f13b9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os,shutil,json\n",
    "import argparse\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "from scipy.special import comb\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-ablation_ratio_test\", type=float, default=0.0107)#for randomized_ablation\n",
    "parser.add_argument(\"-ablation_ratio_test1\", type=float, default=0.0161)#for MMCert\n",
    "parser.add_argument(\"-ablation_ratio_test2\", type=float, default=0.0054)#for MMCert\n",
    "parser.add_argument(\"-r1_r2_ratio\", type=int, default=1)#for MMCert\n",
    "parser.add_argument(\"-alpha\", type=float, default=0.001)\n",
    "parser.add_argument(\"-c\", type=float, help=\"number of test samples\", default=58)\n",
    "parser.add_argument(\"-num_ablated_inputs\", type=int, default=100)\n",
    "\n",
    "def plot_tensor_image(tensor_image, batch_idx=0):\n",
    "    # If tensor has more than 2 dimensions, select the batch index\n",
    "    if len(tensor_image.shape) > 2:\n",
    "        tensor_image = tensor_image[batch_idx]\n",
    "    \n",
    "    # Convert tensor to numpy array\n",
    "    image_array = tensor_image.detach().cpu().numpy()\n",
    "\n",
    "    # Plot the image\n",
    "    plt.imshow(image_array, cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "def _lower_confidence_bound(NA: int, N: int, alpha: float) -> float:\n",
    "        \"\"\" Returns a (1 - alpha) lower confidence bound on a bernoulli proportion.\n",
    "        This function uses the Clopper-Pearson method.\n",
    "        :param NA: the number of \"successes\"\n",
    "        :param N: the number of total draws\n",
    "        :param alpha: the confidence level\n",
    "        :return: a lower bound on the binomial proportion which holds true w.p at least (1 - alpha) over the samples\n",
    "        \"\"\"\n",
    "        return proportion_confint(NA, N, alpha, method=\"beta\")\n",
    "def get_bound_list(args):\n",
    "    l =[]\n",
    "    for i in range(101):\n",
    "        lower,upper = _lower_confidence_bound(i, args.num_ablated_inputs, args.alpha)\n",
    "        l.append((lower,upper))\n",
    "    return l\n",
    "\n",
    "\n",
    "def get_bounds(args,counts,bound_list):\n",
    "    lower_bounds = np.zeros((58,375,1242))\n",
    "    upper_bounds = np.zeros((58,375,1242))\n",
    "    for k in range(58):\n",
    "        for i in range(375):\n",
    "            for j in range(1242):\n",
    "                lower_bounds[k][i][j],upper_bounds[k][i][j] = bound_list[counts[k][i][j]]\n",
    "    return lower_bounds, upper_bounds\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_bounds1(args, counts, bound_list):\n",
    "    bound_array = np.array(bound_list)  # Convert list of tuples to 2D numpy array\n",
    "\n",
    "    # Index into bound_array using counts to fetch the corresponding bounds\n",
    "    lower_bounds = bound_array[counts][:, :, :, 0]\n",
    "    upper_bounds = bound_array[counts][:, :, :, 1]\n",
    "\n",
    "    return lower_bounds, upper_bounds\n",
    "\n",
    "def get_certified_pixels(all_gt,lower_positive,upper_positive,lower_negative,upper_negative):\n",
    "    certified_pixels = np.zeros((58,375,1242))\n",
    "    for k in range(58):\n",
    "        print(k)\n",
    "        for i in range(375):\n",
    "            for j in range(1242):\n",
    "                if all_gt[k][i][j] ==0:\n",
    "                    if lower_negative[k][i][j] > upper_positive[k][i][j]:\n",
    "                        certified_pixels[k][i][j] = 1\n",
    "                    else:\n",
    "                        certified_pixels[k][i][j] = 0\n",
    "                if all_gt[k][i][j] ==1:\n",
    "                    if lower_positive[k][i][j] > upper_negative[k][i][j]:\n",
    "                        certified_pixels[k][i][j] = 1\n",
    "                    else:\n",
    "                        certified_pixels[k][i][j] = 0\n",
    "    return certified_pixels\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_certified_pixels1(all_gt, lower_positive, upper_positive, lower_negative, upper_negative,e1,e2,n1,n2,k1,k2):\n",
    "    # Initialize the tensor with zeros\n",
    "    certified_pixels = np.zeros((58, 375, 1242))\n",
    "    delta = 1-((comb(e1,k1, exact=True)*comb(e2,k2, exact=True))/(comb(n1,k1, exact=True)*comb(n2,k2, exact=True)))\n",
    "    lower_positive= np.array(lower_positive)-delta\n",
    "    upper_positive= np.array(upper_positive)+delta\n",
    "    lower_negative= np.array(lower_negative)-delta\n",
    "    upper_negative= np.array(upper_negative)+delta\n",
    "    # For ground truth values of 0\n",
    "    mask_gt0 = (all_gt == 0)\n",
    "    certified_pixels[mask_gt0] = (lower_negative[mask_gt0] > upper_positive[mask_gt0]).astype(np.float32)\n",
    "\n",
    "    # For ground truth values of 1\n",
    "    mask_gt1 = (all_gt == 1)\n",
    "    certified_pixels[mask_gt1] = (lower_positive[mask_gt1] > upper_negative[mask_gt1]).astype(np.float32)\n",
    "    \n",
    "    return certified_pixels\n",
    "\n",
    "    \n",
    "def calculate_certified_recall(all_gt, certified_pixels):\n",
    "    # Ensure inputs are torch tensors\n",
    "    all_gt = torch.tensor(all_gt)\n",
    "    certified_pixels = torch.tensor(certified_pixels)\n",
    "\n",
    "    # Calculate True Positives\n",
    "    TP = torch.sum((all_gt == 1) & (certified_pixels == 1))\n",
    "\n",
    "    # Calculate False Positives\n",
    "    FN = torch.sum((all_gt == 1) & (certified_pixels == 0))\n",
    "\n",
    "    # Handle the case where TP + FP = 0 to avoid division by zero\n",
    "    if TP + FN == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate precision\n",
    "    recall = TP.float() / (TP + FN).float()\n",
    "\n",
    "    return recall.item()  # return as a scalar\n",
    "def calculate_certified_metrics(all_gt, certified_pixels):\n",
    "    # Ensure inputs are torch tensors\n",
    "    all_gt = torch.tensor(all_gt)\n",
    "    certified_pixels = torch.tensor(certified_pixels)\n",
    "\n",
    "    # Calculate True Positives\n",
    "    TP = torch.sum((all_gt == 1) & (certified_pixels == 1))\n",
    "\n",
    "    # Calculate False Negative\n",
    "    FN = torch.sum((all_gt == 1) & (certified_pixels == 0))\n",
    "    # Calculate False Positive\n",
    "    FP = torch.sum((all_gt == 0) & (certified_pixels == 0))\n",
    "    # Calculate True Negative\n",
    "    TN = torch.sum((all_gt == 0) & (certified_pixels == 1))\n",
    "    # Handle the case where TP + FP = 0 to avoid division by zero\n",
    "\n",
    "    # Calculate precision\n",
    "    pixel_accuracy = (TP.float()+TN.float()) / (TP + FN+FP+TN).float()\n",
    "    recall = TP.float() / (TP + FN).float()\n",
    "    precision = TP.float() / (TP + FP).float()\n",
    "    iou = TP.float() / (TP + FP+FN).float()\n",
    "    return pixel_accuracy.item(), recall.item(), precision.item(), iou.item() # return as a scalar\n",
    "def calculate_certified_per_pixel_accuracy(all_gt, certified_pixels):\n",
    "    # Ensure inputs are torch tensors\n",
    "    all_gt = torch.tensor(all_gt)\n",
    "    certified_pixels = torch.tensor(certified_pixels)\n",
    "    \n",
    "\n",
    "    return certified_pixels.sum().item()/(375*1242)/58  # return as a scalar\n",
    "def get_alpha_list(args,n1,n2,k1,k2,certification_method=\"MMCert\"):\n",
    "    l =np.ones((25,101))\n",
    "    \n",
    "    for r in range(25):\n",
    "        r1 = r\n",
    "        r2 = args.r1_r2_ratio*r\n",
    "        e1 = n1-r1\n",
    "        e2 = n2-r2\n",
    "        if certification_method==\"MMCert\":\n",
    "            delta = 1-((comb(e1,k1, exact=True)*comb(e2,k2, exact=True))/(comb(n1,k1, exact=True)*comb(n2,k2, exact=True)))\n",
    "        else:\n",
    "            delta = 1-((comb(e1+e2,k1+k2, exact=True))/(comb(n1+n2,k1+k2, exact=True)))\n",
    "        for i in range(101):\n",
    "            alpha = 0.00001\n",
    "            for j in range(50):\n",
    "                lower_positive,upper_positive = _lower_confidence_bound(i, 100, alpha)\n",
    "                lower_negative,upper_negative = _lower_confidence_bound(100-i, 100, alpha)\n",
    "                #print((lower_positive-delta), (upper_negative+delta))\n",
    "                if ((lower_positive-delta) > (upper_negative+delta)) or ((lower_negative-delta) > (upper_positive+delta)):\n",
    "                    alpha = alpha/10\n",
    "                else:\n",
    "                    if j ==0:\n",
    "                        l[r][i]=1\n",
    "                    else:\n",
    "                        l[r][i]=alpha\n",
    "                    break\n",
    "    return l\n",
    "def get_all_alpha(all_pred,all_gt,alpha_list,r):\n",
    "    all_alpha = torch.ones(all_pred.shape)\n",
    "    for i in range(58):\n",
    "        for j in range(375):\n",
    "            for k in range(1242):\n",
    "                count = all_pred[i][j][k]\n",
    "                if (all_gt[i][j][k]==1 and count<=50) or (all_gt[i][j][k]==0 and count>=50):\n",
    "                    all_alpha[i][j][k] = 1\n",
    "                else:\n",
    "                    all_alpha[i][j][k]= alpha_list[r][count]\n",
    "    \n",
    "    return all_alpha\n",
    "\n",
    "def get_all_alpha1(all_count, all_gt, alpha_list, r):\n",
    "    all_gt = torch.tensor(all_gt)\n",
    "    all_count = torch.tensor(all_count)\n",
    "    # Create a tensor of ones with the same shape as all_count\n",
    "    all_alpha = torch.ones_like(all_count,dtype=torch.float64)\n",
    "    alpha_list = torch.tensor(alpha_list,dtype=torch.float64) \n",
    "    # Mask for where all_gt is 1 and count in all_pred is <= 50\n",
    "    mask1 = (all_gt == 1) & (all_count <= 50)\n",
    "    \n",
    "    # Mask for where all_gt is 0 and count in all_pred is >= 50\n",
    "    mask2 = (all_gt == 0) & (all_count >= 50)\n",
    "    \n",
    "    # Update all_alpha values to 0 where either mask1 or mask2 is true\n",
    "    all_alpha[mask1 | mask2] = 1\n",
    "\n",
    "    # For other values, get from alpha_list\n",
    "    all_alpha[~(mask1 | mask2)] = alpha_list[r][all_count[~(mask1 | mask2)]]\n",
    "    \n",
    "    return all_alpha\n",
    "\n",
    "def get_certified_pixels_adaptive(all_alpha):\n",
    "    total_budget = 0.001\n",
    "    flattened_alpha = all_alpha.view(all_alpha.size(0), -1)\n",
    "\n",
    "    # Sort the flattened tensor along its last dimension\n",
    "    sorted_values, sorted_indices = torch.sort(flattened_alpha, dim=-1)\n",
    "\n",
    "    cumsum_values = torch.cumsum(sorted_values, dim=-1)\n",
    "\n",
    "    # Find the index where the cumulative sum exceeds the total_budget for each row\n",
    "    #print((cumsum_values <= total_budget).sum(dim=-1).tolist())\n",
    "    mask = (cumsum_values <= total_budget)#.sum(dim=-1).tolist()\n",
    "    B = all_alpha.size(0)\n",
    "    certified_pixels = torch.zeros_like(flattened_alpha)\n",
    "\n",
    "    # Using the mask, get the corresponding indices from sorted_indices for each image in the batch\n",
    "    for i in range(B):\n",
    "        retained_indices = sorted_indices[i][mask[i]]\n",
    "        certified_pixels[i].index_fill_(0, retained_indices, 1)\n",
    "    \n",
    "    return certified_pixels.view_as(all_alpha)\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f22bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========MSCert=========\n",
      "torch.Size([58, 100, 375, 1242])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_gt = torch.tensor(all_gt)\n",
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:202: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_count = torch.tensor(all_count)\n",
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:133: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_gt = torch.tensor(all_gt)\n",
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  certified_pixels = torch.tensor(certified_pixels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r, pixel_acc, recall, precision,iou\n",
      "0 (0.9707775712013245, 0.9100191593170166, 0.9269986152648926, 0.8491644263267517)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "1 (0.9700616598129272, 0.9078407883644104, 0.9251534342765808, 0.8457256555557251)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "2 (0.9692890048027039, 0.9054640531539917, 0.9231821298599243, 0.8420230150222778)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "3 (0.968715488910675, 0.9034517407417297, 0.921931803226471, 0.8392465710639954)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "4 (0.9679130911827087, 0.9009279608726501, 0.919928252696991, 0.835416316986084)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "5 (0.9668677449226379, 0.8973907828330994, 0.9175270199775696, 0.8304075598716736)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "6 (0.9663081169128418, 0.8954989314079285, 0.9162366390228271, 0.8277344107627869)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "7 (0.9651195406913757, 0.8915169835090637, 0.9134615659713745, 0.8220841884613037)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "8 (0.9644742608070374, 0.889386773109436, 0.9119246602058411, 0.8190329670906067)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "9 (0.963104248046875, 0.8849238157272339, 0.9086036682128906, 0.8125914931297302)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "10 (0.9623421430587769, 0.8825044631958008, 0.9066998958587646, 0.80903559923172)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "11 (0.9614648222923279, 0.8797928690910339, 0.9044439196586609, 0.8049696087837219)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "12 (0.9600214958190918, 0.8754096031188965, 0.9006624221801758, 0.7983290553092957)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "13 (0.9589964747428894, 0.8723267912864685, 0.8979490995407104, 0.7936455607414246)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "14 (0.9577541351318359, 0.868687629699707, 0.8945767283439636, 0.7880165576934814)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "15 (0.9563920497894287, 0.8646458983421326, 0.8909176588058472, 0.7818724513053894)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "16 (0.9544966816902161, 0.8590851426124573, 0.885769784450531, 0.7734009027481079)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "17 (0.9512298703193665, 0.849323570728302, 0.8770187497138977, 0.7589360475540161)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "18 (0.946638822555542, 0.8357537388801575, 0.8645637035369873, 0.7390013337135315)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "19 (0.21470531821250916, 0.0, 0.0, 0.0)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "20 (0.021470746025443077, 0.0, 0.0, 0.0)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "21 (0.0021449276246130466, 0.0, 0.0, 0.0)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "22 (0.0021449276246130466, 0.0, 0.0, 0.0)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "23 (0.0, 0.0, 0.0, 0.0)\n",
      "r, pixel_acc, recall, precision,iou\n",
      "24 (0.0, 0.0, 0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"========MSCert=========\")\n",
    "n1 = 375*1242\n",
    "n2 = 375*1242\n",
    "k1 = int(n1*args.ablation_ratio_test1)\n",
    "k2 = int(n2*args.ablation_ratio_test2)\n",
    "all_outputs = torch.load(\"output\\\\\"+\"MMCert\"+\"_ablation-ratio-test1=\"+str(args.ablation_ratio_test1)+\"_ablation-ratio-test2=\"+str(args.ablation_ratio_test2)+\"_all_outputs.pth\")\n",
    "all_pred =all_outputs[\"all_pred\"]\n",
    "all_gt = all_outputs[\"all_gt\"]\n",
    "all_pred = torch.stack([torch.stack(tensors) for tensors in all_pred])\n",
    "all_gt = torch.stack([torch.stack(tensors) for tensors in all_gt])\n",
    "all_pred = torch.transpose(all_pred, 0, 1)\n",
    "all_gt = all_gt[0]\n",
    "\n",
    "#print(all_globalacc[0])\n",
    "#print(torch.min(all_iou[0]),torch.max(all_iou[0]))\n",
    "#{\"all_globalacc\":all_globalacc, \"all_pre\":all_pre, \"all_recall\":all_recall, \"all_F_score\":all_F_score, \"all_iou\":all_iou}\n",
    "all_count = torch.sum(all_pred, dim=1)\n",
    "all_predictions = (all_count > 50).int()\n",
    "print(all_pred.shape)\n",
    "\n",
    "alpha_list = get_alpha_list(args,n1,n2,k1,k2, certification_method=\"MMCert\")\n",
    "certified_metrics_mmcert = []\n",
    "for r in range(25):\n",
    "    all_alpha = get_all_alpha1(all_count, all_gt, alpha_list,r)\n",
    "    #print(all_alpha)\n",
    "    certified_pixels = get_certified_pixels_adaptive(all_alpha)\n",
    "    certified_metrics_mmcert.append(calculate_certified_metrics(all_gt, certified_pixels))\n",
    "    #plot_tensor_image(certified_pixels,2)\n",
    "    print(\"r, pixel_acc, recall, precision,iou\")\n",
    "    print(r,calculate_certified_metrics(all_gt, certified_pixels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========randomized_ablation=========\n",
      "torch.Size([58, 100, 375, 1242])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_gt = torch.tensor(all_gt)\n",
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:202: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_count = torch.tensor(all_count)\n",
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:133: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_gt = torch.tensor(all_gt)\n",
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\386582063.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  certified_pixels = torch.tensor(certified_pixels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r, pixel_acc, recall, precision,iou\n",
      "0 (0.9668641090393066, 0.8817130327224731, 0.9313355684280396, 0.827894926071167)\n"
     ]
    }
   ],
   "source": [
    "print(\"========randomized_ablation=========\")\n",
    "n1 = 375*1242\n",
    "n2 = 375*1242\n",
    "k1 = int(n1*args.ablation_ratio_test)\n",
    "k2 = int(n2*args.ablation_ratio_test)\n",
    "all_outputs = torch.load(\"output\\\\\"+\"randomized_ablation\"+\"_ablation-ratio-test=\"+str(args.ablation_ratio_test)+\"_all_outputs.pth\")\n",
    "all_pred =all_outputs[\"all_pred\"]\n",
    "all_gt = all_outputs[\"all_gt\"]\n",
    "all_pred = torch.stack([torch.stack(tensors) for tensors in all_pred])\n",
    "all_gt = torch.stack([torch.stack(tensors) for tensors in all_gt])\n",
    "all_pred = torch.transpose(all_pred, 0, 1)\n",
    "all_gt = all_gt[0]\n",
    "\n",
    "#print(all_globalacc[0])\n",
    "#print(torch.min(all_iou[0]),torch.max(all_iou[0]))\n",
    "#{\"all_globalacc\":all_globalacc, \"all_pre\":all_pre, \"all_recall\":all_recall, \"all_F_score\":all_F_score, \"all_iou\":all_iou}\n",
    "all_count = torch.sum(all_pred, dim=1)\n",
    "all_predictions = (all_count > 50).int()\n",
    "print(all_pred.shape)\n",
    "\n",
    "alpha_list = get_alpha_list(args,n1,n2,k1,k2, certification_method=\"randomized_ablation\")\n",
    "certified_metrics_ra = []\n",
    "for r in range(25):\n",
    "    all_alpha = get_all_alpha1(all_count, all_gt, alpha_list,r)\n",
    "    #print(all_alpha)\n",
    "    certified_pixels = get_certified_pixels_adaptive(all_alpha)\n",
    "    #plot_tensor_image(certified_pixels,2)\n",
    "    certified_metrics_ra.append(calculate_certified_metrics(all_gt, certified_pixels))\n",
    "    print(\"r, pixel_acc, recall, precision,iou\")\n",
    "    print(r,calculate_certified_metrics(all_gt, certified_pixels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7e3889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========vanilla=========\n",
      "torch.Size([58, 375, 1242])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongs\\AppData\\Local\\Temp\\ipykernel_30532\\78833124.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_gt = torch.tensor(all_gt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9015240669250488\n",
      "1 0.8980993032455444\n",
      "2 0.8944099545478821\n",
      "3 0.8924378156661987\n",
      "4 0.8882236480712891\n",
      "5 0.8859847187995911\n",
      "6 0.8811527490615845\n",
      "7 0.8785921335220337\n",
      "8 0.8758500218391418\n",
      "9 0.8693690896034241\n",
      "10 0.8654698729515076\n",
      "11 0.8609969019889832\n",
      "12 0.8552723526954651\n",
      "13 0.8472498655319214\n",
      "14 0.8472498655319214\n",
      "15 0.8353611826896667\n",
      "16 0.813547670841217\n",
      "17 0.813547670841217\n",
      "18 0.0\n",
      "19 0.0\n",
      "20 0.0\n",
      "21 0.0\n",
      "22 0.0\n",
      "23 0.0\n",
      "24 0.0\n",
      "25 0.0\n",
      "26 0.0\n",
      "27 0.0\n",
      "28 0.0\n",
      "29 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"==========vanilla=========\")\n",
    "positive_counts = all_count\n",
    "negative_counts = 100- all_count\n",
    "args.alpha = 0.001/n1\n",
    "print(positive_counts.shape)\n",
    "bound_list = get_bound_list(args)\n",
    "lower_positive, upper_positive = get_bounds1(args,positive_counts,bound_list)\n",
    "lower_negative, upper_negative = get_bounds1(args,negative_counts,bound_list)\n",
    "rs = []\n",
    "CAs = []\n",
    "CAs_baseline = []\n",
    "RANGE=30\n",
    "for r in range(RANGE):\n",
    "    r1 = r\n",
    "    r2 = args.r1_r2_ratio*r\n",
    "    e1 = n1-r1\n",
    "    e2 = n2-r2\n",
    "    certified_pixels= get_certified_pixels1(all_gt,lower_positive,upper_positive,lower_negative,upper_negative,e1,e2,n1,n2,k1,k2)\n",
    "    print(r,calculate_certified_recall(all_gt, certified_pixels))\n",
    "    ca = certified_pixels.sum().item()/(375*1242)/58\n",
    "    #print(r, ca)\n",
    "    CAs.append(ca)\n",
    "    rs.append(r)\n",
    "\n",
    "#certified_pixels= get_certified_pixels1(all_gt,lower_positive,upper_positive,lower_negative,upper_negative,e1,e2,n1,n2,k1,k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90bd800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.006433946195449594, 0.16361499252741493)\n",
      "(0.836385007472585, 0.9935660538045504)\n"
     ]
    }
   ],
   "source": [
    "print(_lower_confidence_bound(5, 100, 0.001))\n",
    "print(_lower_confidence_bound(95, 100, 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "647bba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9689200214707461\n"
     ]
    }
   ],
   "source": [
    "print(certified_pixels.sum().item()/(58*375*1242))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b1b7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(lower_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b12887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
